<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Transformer | Dekel'Blog</title><meta name="author" content="Dekel,tangxk666@outlook.com"><meta name="copyright" content="Dekel"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="关于Transformer架构的学习">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer">
<meta property="og:url" content="https://dekelkai.github.io/2025/01/27/Transformer/index.html">
<meta property="og:site_name" content="Dekel&#39;Blog">
<meta property="og:description" content="关于Transformer架构的学习">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/20250930204459119.png">
<meta property="article:published_time" content="2025-01-27T14:11:34.000Z">
<meta property="article:modified_time" content="2025-09-17T08:45:49.800Z">
<meta property="article:author" content="Dekel">
<meta property="article:tag" content="学习">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/20250930204459119.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Transformer",
  "url": "https://dekelkai.github.io/2025/01/27/Transformer/",
  "image": "https://cdn.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/20250930204459119.png",
  "datePublished": "2025-01-27T14:11:34.000Z",
  "dateModified": "2025-09-17T08:45:49.800Z",
  "author": [
    {
      "@type": "Person",
      "name": "Dekel",
      "url": "https://Dekelkai.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/20250930202811272.jpg"><link rel="canonical" href="https://dekelkai.github.io/2025/01/27/Transformer/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Transformer',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="https://cdn.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/20250930204459119.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">21</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo/"><i class="fa-fw fas fa-video"></i><span> 说说</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src="https://cdn.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/20250930204459119.png" alt="Logo"><span class="site-name">Dekel'Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">Transformer</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo/"><i class="fa-fw fas fa-video"></i><span> 说说</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Transformer</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-01-27T14:11:34.000Z" title="发表于 2025-01-27 22:11:34">2025-01-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-09-17T08:45:49.800Z" title="更新于 2025-09-17 16:45:49">2025-09-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0/Transformer/">Transformer</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h4 id="编码器层encoder-layer">编码器层（Encoder Layer）</h4>
<blockquote>
<p>编码器层的输入首先进入自注意力子层（Self-Attention），该子层的作用在于帮助编码器关注句子中的其他词汇，以便更好地编码某个特定词汇。</p>
<p>随后，自注意力子层的输出将传递给一个前馈神经网络（Feed-Forward Neural
Network）。结构完全相同的前馈网络被独立地应用于每个位置。</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/Transformer_encoder.png" alt="img"></p>
<blockquote>
<p>输入输出对理解数据流非常重要。编码器层的输入形状为 S x
D（请参见下面的图表），其中 S 是源句子长度（例如，英语句子），而 D
是嵌入的维度（也是模型维度，论文中取值为 512）。</p>
<p>编码器的输入和输出形状相同。由于编码器层是相互叠加的，因此，我们希望其输出具有与输入相同的维度，以便它可以轻松地流入下一个编码器层。因此，输出也是
S x D 形状。</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/encoder-layer-io.jpeg" alt="img"></p>
<h4 id="解码器层decoder-layer">解码器层（Decoder Layer）</h4>
<blockquote>
<p>解码器层（decoder
layer）也包含前面编码器中提到的两个层，不过区别在于这两个层之间还夹了一个注意力层（Encoder-Decoder
Attention）。这个额外的注意力层的作用在于让解码器能够注意到输入句子中与解码任务相关的部分。</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/Transformer_decoder.png" alt="img"></p>
<blockquote>
<p><strong>在一个已经训练好的Transformer模型中</strong>，输入是怎么变为输出的呢？首先我们要知道各种各样的张量（向量）是如何在这些组件之间变化的。</p>
<p>与其他的NLP项目一样，我们首先需要把输入的每个单词通过词嵌入（embedding）转化为对应的向量。</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/embeddings.png" alt="img"></p>
<blockquote>
<p>所有编码器层接收一组向量作为输入（论文中的输入向量的维度是512）。最底下的那个编码器层接收的是嵌入向量，之后的编码器层接收的是前一个编码器层的输出。</p>
<p>向量列表的长度这个超参数是我们可以设置的，一般来说是我们训练集中最长的那个句子的长度。</p>
<p>当我们的输入序列经过词嵌入之后得到的向量会依次通过编码器层中的两个层。</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/encoder_with_tensors.png" alt="img"></p>
<h3 id="注意力机制attention">注意力机制（Attention）</h3>
<blockquote>
<p>注意力机制是论文的核心，它在编码器和解码器部分的处理稍有差异。让我们先以编码器部分的注意力层机制为例进行介绍。</p>
<p>上边提到，每个编码器层接受一组向量作为输入。在其内部，输入向量先通过一个自注意力层，再经过一个前馈神经网络层，最后将其将输出给下一个编码器层。</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/encoder_with_tensors_2.png" alt="img"></p>
<blockquote>
<p>不同位置上的单词都要经过自注意力层的处理，之后都会经过一个完全相同的前馈神经网络。</p>
<p>在这里，我们开始看到 Transformer
的一个关键特点，即每个位置上的单词在编码器层中有各自的流通方向。</p>
<ol type="1">
<li><strong>在自注意力层中，这些路径之间存在依赖关系。</strong>单词和单词之间会有关联，假设一个句子有
50 个单词，那么可以粗略想象成自注意力计算过程中，会构造一个 50 x 50
的关联矩阵。</li>
<li>前馈神经网络（Feed
Forward）层中<strong>没有</strong>这些依赖关系。每个单词独立通过前馈神经网络，单词和单词之间没有关联，因此各种路径可以在流过前馈网络层的时候并行计算。</li>
</ol>
</blockquote>
<h4 id="自注意力self-attention">自注意力（Self-Attention）</h4>
<blockquote>
<p>现在让我们看一下自注意力机制。</p>
<p>假设我们要翻译下边这句话：<br>
<code>”The animal didn't cross the street because it was too tired”</code></p>
<p>这里<code>it</code>指的是什么？是<code>street</code>还是<code>animal</code>？人理解起来很容易，但是对算法来讲就不那么容易了。</p>
<p>当模型处理<code>it</code>这个词的时候，自注意力会让<code>it</code>和<code>animal</code>关联起来。</p>
<p>当模型编码每个位置上的单词的时候，自注意力的作用就是：看一看输入句子中其他位置的单词，试图寻找一种对当前单词更好的编码方式。</p>
<p>如果熟悉 RNNs 模型，回想一下 RNN
如何处理当前时间步的隐藏状态：将之前的隐藏状态与当前位置的输入结合起来。在
Transformer
中，自注意力机制也可以将其他相关单词的“理解”融入到我们当前处理的单词中。</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/transformer_self-attention_visualization.png" alt="img"></p>
<p>当我们在最后一个encoder组建中对it进行编码的时候，注意力机制会更关注The
animal，并将其融入到it的编码中。</p>
<h4 id="自注意力的计算单个">自注意力的计算（单个）</h4>
<blockquote>
<p>先画图用向量解释一下自注意力是怎么算的，之后再看一下实际实现中是怎么用矩阵算的。</p>
<p><strong>第一步</strong>
对于编码器的每个输入向量x，都会计算三个向量，即query、key和value向量。</p>
<p>这些向量的计算方法是将输入的词嵌入向量与三个权重矩阵相乘。这些权重矩阵是在模型训练阶段通过训练得到的。</p>
<p>什么是 “query”、“key”、“value”
向量？这三个向量是计算注意力时的抽象概念，请继续往下看注意力计算过程。</p>
<p><strong>第二步</strong> 计算注意力得分。</p>
<p>假设我们现在在计算输入中第一个单词 <code>Thinking</code>
的自注意力。我们需要使用自注意力给输入句子中的每个单词打分，这个分数决定当我们编码某个位置的单词的时候，应该对其他位置上的单词给予多少关注度。</p>
<p>这个得分是query和key的点乘积得出来的。例如，如果我们处理位置#1的单词的自我注意，第一个分数将是q1和k1的点积。第二个分数是q1和k2的点积。（备注：在使用矩阵处理时，是用
Q 和 K 的转置相乘得到，详见后）。</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/transformer_self_attention_score.png" alt="img"></p>
<blockquote>
<p><strong>第三步</strong> 将计算获得的注意力分数除以 8。</p>
<p>为什么选 8？是因为key向量的维度是
64，取其平方根，这样让梯度计算的时候更稳定。默认是这么设置的，当然也可以用其他值。</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/self-attention_softmax.png" alt="img"></p>
<blockquote>
<p><strong>第四步</strong> 除 8 之后将结果扔进 softmax
计算，使结果归一化，softmax 之后注意力分数相加等于 1，并且都是正数。</p>
<p>这个 softmax 之后的注意力分数表示
在计算当前位置的时候，其他单词受到的关注度的大小。显然在当前位置的单词肯定有一个高分，但是有时候也会注意到与当前单词相关的其他词汇。</p>
<p><strong>第五步</strong> 将每个 value
向量乘以注意力分数。这是为了强化我们想要关注的单词的
value，并尽量抑制其他不相关的单词（通过乘以一个接近于零的数，如
0.001）。这个过程被称为“缩放”或者“加权”，可以使得我们更加关注与目标单词相关的单词。</p>
<p><strong>第六步</strong>
将上一步的结果相加，输出本位置的注意力结果。</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/self-attention-output.png" alt="img"></p>
<p>这就是自注意力的计算。计算得到的向量直接传递给前馈神经网络。但是为了处理的更迅速，实际是用矩阵进行计算的。接下来我们看一下怎么用矩阵计算。</p>
<h4 id="自注意力的计算矩阵">自注意力的计算（矩阵）</h4>
<blockquote>
<p>第一步是计算 Query、Key 和 Value 矩阵。我们通过将嵌入打包到矩阵 X
中，并将其乘以我们训练的权重矩阵<span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="18.254ex" height="2.626ex" role="img" focusable="false" viewBox="0 -910.8 8068.3 1160.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(389,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(1136.2,413) scale(0.707)"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g></g><g data-mml-node="mtext" transform="translate(2134.5,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">、</text></g><g data-mml-node="msup" transform="translate(3134.5,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(1136.2,413) scale(0.707)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g><g data-mml-node="mtext" transform="translate(4949.3,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">、</text></g><g data-mml-node="msup" transform="translate(5949.3,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(1136.2,413) scale(0.707)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g><g data-mml-node="mo" transform="translate(7679.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span>来实现这一点。</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/self-attention-matrix-calculation.png" alt="img"></p>
<p>​ X矩阵中的每一行对应于输入句子中的一个单词。<br>
​可再次看到嵌入向量维度（512，图中的 4 个框）和 q/k/v
向量维度（64，图中的 3 个框）的差异</p>
<blockquote>
<p>最后，由于我们使用矩阵计算，因此可以将步骤 2 到 6
合并为一个公式，以计算自注意力层的输出。</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/self-attention-matrix-calculation-2.png" alt="img"></p>
<blockquote>
<p>通过将输入向量 x 与注意力头的权重矩阵相乘，可以得到对应的 query、key
和 value
向量。单个头获取的这三个向量维度是64，比嵌入向量的维度小，8个头的输出连接后变为
512。因此嵌入向量、编码器层的输入输出维度都是512。</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/self-attention-layer.jpeg" alt="img"></p>
<p>对于上图的解释：</p>
<ol type="1">
<li>假定输入的英文句子是“The quick brown fox“，句子长度 S
为4，参考“编码器层”章节的解释，注意力子层的输入形状为（4 x 512）。</li>
<li>自注意力层使用三个权重矩阵进行初始化——Query（W<sup>q）、Key（W</sup>k）和Value（W<sup>v</sup>）。这些权重矩阵的尺寸都是
D x d，在论文中d取值为64，即权重矩阵的尺寸为 512 x
64。在训练模型时，我们将训练这些矩阵的权重。</li>
<li>在第一次计算（图中的Calc
1）中，我们通过将输入（注意：代码实现中是三个不同的输入，编码器层都是X，解码器层不同，见代码中的解释）与各自的Query、Key和Value权重矩阵相乘，计算出Q、K和V矩阵（尺寸为
S x d，示例中为 4 x 64）。</li>
<li>在第二次计算中，参考Attention计算公式，首先将Q和Kᵀ矩阵相乘，得到一个尺寸为
S x S（示例中为 4 x
4）的矩阵，然后将其除以√d的标量。然后对矩阵进行softmax运算，使得每一行的和都为1。这个矩阵可以理解为句子中每个词之间的关联度。</li>
<li>上面 S x S 的矩阵再和V矩阵相乘，得到尺寸为 S x d（示例中为 4 x
64）的矩阵。经过后续的连接操作后，传入下一层。</li>
</ol>
<h4 id="多头注意力">多头注意力</h4>
<blockquote>
<p>论文进一步改进了自注意力层，增加了一个机制，也就是多头注意力机制。这样做有两个好处：</p>
<p>第一个好处，它扩展了模型专注于不同位置的能力。</p>
<p>在上面例子里只计算一个自注意力的的例子中，编码“Thinking”的时候，虽然最后
Z1
或多或少包含了其他位置单词的信息，但是它实际编码中还是被“Thinking”单词本身所支配。</p>
<p>如果我们翻译一个句子，比如“The animal didn’t cross the street because
it was too
tired”，我们会想知道“it”指的是哪个词，这时模型的“多头”注意力机制会起到作用。</p>
<p>第二个好处，它给了注意层多个“表示子空间”。</p>
<p>就是在多头注意力中同时用多个不同的 WV*W**V* 权重矩阵（Transformer
使用8个头部，因此我们最终会得到8个计算结果)，每个权重都是随机初始化的。经过训练每个
WV*W**V* 都能将输入的矩阵投影到不同的表示子空间。</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/transformer_attention_heads_qkv.png" alt="img"></p>
<p>如果我们做和上面相同的自注意力计算，只不过八次使用不同的权重矩阵，我们最后得到八个不同的Z矩阵。</p>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/transformer_attention_heads_z.png" alt="img"></p>
<blockquote>
<p>但是这会存在一点问题，多头注意力出来的结果会进入一个前馈神经网络，这个前馈神经网络可不能一下接收8个注意力矩阵，它的输入需要是单个矩阵（矩阵中每个行向量对应一个单词），所以我们需要一种方法把这8个压缩成一个矩阵。</p>
<p>怎么做呢？我们将这些矩阵连接起来，然后将乘以一个附加的权重矩阵</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/transformer_attention_heads_weight_matrix_o.png" alt="img"></p>
<p>以上就是多头自注意力的全部内容。让我们把多头注意力上述内容
放到一张图里看一下子：</p>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/transformer_multi-headed_self-attention-recap.png" alt="img"></p>
<blockquote>
<p>现在我们已经看过什么是多头注意力了，让我们回顾一下之前的一个例子，再看一下编码“it”的时候每个头的关注点都在哪里：</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/transformer_self-attention_visualization_2.png" alt="img"></p>
<blockquote>
<p>如果我们把所有的头的注意力都可视化一下，就是下图这样，但是看起来事情好像突然又复杂了。</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/transformer_self-attention_visualization_3.png" alt="img"></p>
<h3 id="编码器encoder">编码器（Encoder）</h3>
<h4 id="使用位置编码表示序列的位置">使用位置编码表示序列的位置</h4>
<blockquote>
<p>到现在我们还没提到过如何表示输入序列中词汇的位置。</p>
<p>Transformer
在每个输入的嵌入向量中添加了位置向量。这些位置向量遵循某些特定的模式，这有助于模型确定每个单词的位置或不同单词之间的距离。将这些值添加到嵌入矩阵中，一旦它们被投射到Q、K、V中，就可以在计算点积注意力时提供有意义的距离信息。</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/transformer_positional_encoding_vectors.png" alt="img"></p>
<p>位置编码向量和嵌入向量的维度是一样的，比如下边都是四个格子：</p>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/transformer_positional_encoding_example.png" alt="img"></p>
<blockquote>
<p><strong>举个例子，当嵌入向量的长度为4的时候，位置编码长度也是4</strong></p>
<p>一直说位置向量遵循某个模式，这个模式到底是什么。</p>
<p>参考论文：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.03122">Convolutional
Sequence to Sequence Learning</a> <img src="https://www.vectorexplore.com/s2citation/arXiv:1705.03122" alt="论文 arXiv:1705.03122 的 Semantic Scholar 引用数"></p>
<p>在下面的图中，每一行对应一个位置编码。所以第一行就是我们输入序列中第一个单词的位置编码，之后我们要把它加到词嵌入向量上。</p>
<p>看个可视化的图,<em>这里表示的是一个句子有20个词，词嵌入向量的长度为512。可以看到图像从中间一分为二，因为左半部分是由正弦函数生成的。右半部分由余弦函数生成。然后将它们二者拼接起来，形成了每个位置的位置编码。</em>：</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/transformer_positional_encoding_large_example.png" alt="img"></p>
<blockquote>
<p>但是需要注意注意一点，上图的可视化是官方Tensor2Tensor库中的实现方法，将sin和cos拼接起来。但是和论文原文写的不一样，论文原文的3.5节写了位置编码的公式，论文不是将两个函数concat起来，而是将sin和cos交替使用。论文中公式的写法可以看这个代码：<a target="_blank" rel="noopener" href="https://github.com/jalammar/jalammar.github.io/blob/master/notebookes/transformer/transformer_positional_encoding_graph.ipynb">transformer_positional_encoding_graph</a>，其可视化结果如下：</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/attention-is-all-you-need-positional-encoding.png" alt="img"></p>
<h4 id="全连接的前馈网络feed-forward-networks">全连接的前馈网络（Feed-Forward
Networks）</h4>
<blockquote>
<p>除了注意力子层外，我们的编码器和解码器中的每一层都包含一个全连接的前馈网络，该网络被单独且相同地应用于每个位置。这包括两个线性变换，它们之间有ReLU激活函数。</p>
</blockquote>
<p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="36.197ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 15999 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z"></path><path data-c="46" d="M128 619Q121 626 117 628T101 631T58 634H25V680H582V676Q584 670 596 560T610 444V440H570V444Q563 493 561 501Q555 538 543 563T516 601T477 622T431 631T374 633H334H286Q252 633 244 631T233 621Q232 619 232 490V363H284Q287 363 303 363T327 364T349 367T372 373T389 385Q407 403 410 459V480H450V200H410V221Q407 276 389 296Q381 303 371 307T348 313T327 316T303 317T284 317H232V189L233 61Q240 54 245 52T270 48T333 46H360V0H348Q324 3 182 3Q51 3 36 0H25V46H58Q100 47 109 49T128 61V619Z" transform="translate(653,0)"></path><path data-c="4E" d="M42 46Q74 48 94 56T118 69T128 86V634H124Q114 637 52 637H25V683H232L235 680Q237 679 322 554T493 303L578 178V598Q572 608 568 613T544 627T492 637H475V683H483Q498 680 600 680Q706 680 715 683H724V637H707Q634 633 622 598L621 302V6L614 0H600Q585 0 582 3T481 150T282 443T171 605V345L172 86Q183 50 257 46H274V0H265Q250 3 150 3Q48 3 33 0H25V46H42Z" transform="translate(1306,0)"></path></g><g data-mml-node="mo" transform="translate(2056,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g></g><g data-mml-node="mi" transform="translate(2445,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(3017,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3683.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(4739.6,0)"><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z"></path><path data-c="61" d="M137 305T115 305T78 320T63 359Q63 394 97 421T218 448Q291 448 336 416T396 340Q401 326 401 309T402 194V124Q402 76 407 58T428 40Q443 40 448 56T453 109V145H493V106Q492 66 490 59Q481 29 455 12T400 -6T353 12T329 54V58L327 55Q325 52 322 49T314 40T302 29T287 17T269 6T247 -2T221 -8T190 -11Q130 -11 82 20T34 107Q34 128 41 147T68 188T116 225T194 253T304 268H318V290Q318 324 312 340Q290 411 215 411Q197 411 181 410T156 406T148 403Q170 388 170 359Q170 334 154 320ZM126 106Q126 75 150 51T209 26Q247 26 276 49T315 109Q317 116 318 175Q318 233 317 233Q309 233 296 232T251 223T193 203T147 166T126 106Z" transform="translate(833,0)"></path><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(1333,0)"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(6767.2,0)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g></g><g data-mml-node="mn" transform="translate(7156.2,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(7656.2,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(8100.9,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="msub" transform="translate(8672.9,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(10275.7,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="msub" transform="translate(11275.9,0)"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mn" transform="translate(462,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(12141.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="msub" transform="translate(12530.4,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mn" transform="translate(977,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(14133.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="msub" transform="translate(15133.4,0)"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mn" transform="translate(462,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container></span></p>
<blockquote>
<p>虽然FFN的网络架构在各个位置上都是相同的，但它们在每个位置使用的是不同的权重参数。这可能就是论文作者为了强调这个，加上PositionWise的原因。</p>
<p>另一种描述方法是，这是两个具有1内核大小的卷积。输入和输出的维度是dmodel=512，而内部层的维度是
dff=2048。</p>
</blockquote>
<h4 id="子层之间的连接残差和层归一化">子层之间的连接（残差和层归一化）</h4>
<blockquote>
<h5 id="原始论文">原始论文</h5>
<p>在继续往下讲之前，我们还需再提一下编码器层中的一个细节：每个编码器层中的每个子层（自注意力层、前馈神经网络）都有一个残差连接（图中的Add），之后是做了一个层归一化（layer-normalization）（图中的Normalize）。</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/transformer_resideual_layer_norm.png" alt="img"></p>
<blockquote>
<p>将过程中的向量相加和layer-norm可视化如下所示：</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/transformer_resideual_layer_norm_2.png" alt="img"></p>
<blockquote>
<p>当然在解码器子层中也是这样的。</p>
<p>我们现在画一个有两个编码器和解码器的Transformer，那就是下图这样的：</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/transformer_resideual_layer_norm_3.png" alt="img"></p>
<h3 id="解码器decoder">解码器（Decoder）</h3>
<blockquote>
<p>现在我们已经介绍了编码器的大部分概念，因为Encoder的Decoder差不多，我们基本上也知道了解码器是如何工作的。那让我们直接看看二者是如何协同工作的。</p>
<p>解码器首先处理输入序列，将最后一个编码器层的输出转换为一组注意向量K和V。<code>注意：参考实现中为直接用，见EncoderDecoder.forward，DecoderLayer.forward。</code></p>
<p>每个解码器层将在“encoder-decoder
attention”层中使用编码器传过来的K和V，这有助于解码器将注意力集中在输入序列中的适当位置：</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/transformer_decoding_1.gif" alt="img"></p>
<blockquote>
<p>输出步骤会一直重复，直到遇到句子结束符
表明transformer的解码器已完成输出。</p>
<p>每一步的输出都会在下一个时间步喂给给底部解码器，解码器会像编码器一样运算并输出结果（每次往外蹦一个词）。</p>
<p>跟编码器一样，在解码器中我们也为其添加位置编码，以指示每个单词的位置。</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/transformer_decoding_2.gif" alt="img"></p>
<blockquote>
<p>解码器中的自注意力层和编码器中的不太一样：
在解码器中，自注意力层只允许关注已输出位置的信息。实现方法是在自注意力层的softmax之前进行mask，将未输出位置的信息设为极小值。</p>
<p>“encoder-decoder
attention”层的工作原理和前边的多头自注意力差不多，但是Q、K、V的来源不用，Q是从下层创建的（比如解码器的输入和下层decoder组件的输出），但是其K和V是来自编码器最后一个组件的输出结果。</p>
</blockquote>
<h3 id="最后的线性层和softmax层">最后的线性层和softmax层</h3>
<blockquote>
<p>Decoder输出的是一个浮点型向量（512维），如何把它变成一个词？</p>
<p>这就是最后一个线性层和softmax要做的事情。</p>
<p>线性层就是一个简单的全连接神经网络，它将解码器生成的向量映射到logits向量中。假设我们的模型词汇表是10000个英语单词，它们是从训练数据集中学习的。那logits向量维数也是10000，每一维对应一个单词的分数。</p>
<p>然后，softmax层将这些分数转化为概率（全部为正值，加起来等于1.0），选择其中概率最大的位置的词汇作为当前时间步的输出。</p>
</blockquote>
<p><img src="https://gcore.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/transformer_decoder_output_softmax.png" alt="img"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://Dekelkai.github.io">Dekel</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://dekelkai.github.io/2025/01/27/Transformer/">https://dekelkai.github.io/2025/01/27/Transformer/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://Dekelkai.github.io" target="_blank">Dekel'Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0/">学习</a><a class="post-meta__tags" href="/tags/Transformer/">Transformer</a></div><div class="post-share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/20250930204459119.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/01/29/Vision%20Transformer/" title="Vision Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Vision Transformer</div></div><div class="info-2"><div class="info-item-1">VIT模型整体框架✨✨✨   从整体上来看，VIT模型的结构是很少的，事实上确实如此。如果你明白了我上一篇讲解的Transformer的话，那这篇文章真的就特别简单了，可以说没什么难点。这篇文章作者企图不改变Transformer的结构来实现物体分类任务，我们可以来看一下VIT中的Transformer Encoder 结构，基本是和Transformer中是一样的。注意我这里说的是基本喔，你对比两篇论文中Encoder的结构你会发现，Norm这个结构的位置是有所变化的，至于为什么这样做，作者也没有提及，个人感觉这个改变对结构影响不会很大，感兴趣的可以改变这个结构尝试尝试效果。另外一点是在VIT中没有使用Decoder结构，这里大家需要注意一下。  VIT细节梳理✨✨✨      首先，想想NLP中的Transformer和CV中的VIT这两个结构输入有什么区别？从变量的类型来看，两者都是一个tensor张量；而从变量的维度来看，NLP中的输入往往是二维的tensor，而CV中往往是一个三维的RGB图像。【都忽略Batch维度】 这种维度的不统一会导致我们不能...</div></div></div></a><a class="pagination-related" href="/2025/01/15/%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0-%E7%BD%91%E9%A1%B5%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90%E8%AF%84%E8%AE%BA%E6%8A%93%E5%8F%96/" title="爬虫-网页网易云音乐评论抓取"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">爬虫-网页网易云音乐评论抓取</div></div><div class="info-2"><div class="info-item-1"> 傍晚时分，坐在屋檐下，看着天慢慢地黑下去，心里寂寞而凄凉，感到自己的生命被剥夺了。当时我是个年轻人，但我害怕这样生活下去，衰老下去。在我看来，这是比死亡更可怕的事。 一、准备工作 1. 导入所需的库 1234567import jsonimport randomimport requests# 实现AES加密需要的三个模块from Crypto.Cipher import AES  # AES加密from Crypto.Util.Padding import padfrom base64 import b64encode 2. 捕获评论数据包 网站的评论区数据是通过js动态加载的，在XHR中捕获所有数据包，找到评论区数据包。  3. 分析数据包 在标头可以看到是通过post请求发送的数据，载荷中的表单数据能看出params和encSecKey的值是被加密过的。使用正常方式带上者两个发送post请求是肯定得不到评论数据的，因此必须破解加密。   123456# 请求评论的url地址及发送的请求数据，空串为我们后面需要破解然后填入的位置url = "http...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/02/06/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%AD%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E5%8E%9F%E5%9B%A0/" title="🥸反向传播中梯度消失和梯度爆炸的原因🥸"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-06</div><div class="info-item-2">🥸反向传播中梯度消失和梯度爆炸的原因🥸</div></div><div class="info-2"><div class="info-item-1"> 在深度学习中，梯度消失和梯度爆炸是训练深层神经网络时常见的挑战。要真正理解这些问题，必须深入理解反向传播的机制，尤其是梯度是如何通过链式法则逐层传播的。本文通过手推反向传播的数学推导，帮助理解梯度消失和梯度爆炸的根本原因。  在深度学习中，梯度消失和梯度爆炸是训练深层神经网络时常见的挑战。要真正理解这些问题，必须深入理解反向传播的机制，尤其是梯度是如何通过链式法则逐层传播的。本文通过手推反向传播的数学推导，帮助读者理解梯度消失和梯度爆炸的根本原因。 准备工作  在开始反向传播的推导之前，我们需要明确网络的结构和损失函数的定义。假设我们有一个简单的神经网络，包含两个输入、两个隐藏层神经元和两个输出神经元。每个神经元的输出通过sigmoid函数进行激活，损失函数为均方误差。  每个神经元：  其中为sigmoid函数。  损失函数：  反向传播  反向传播是神经网络训练的核心算法，它通过链式法则计算每一层的梯度，并根据梯度更新网络参数。为了理解梯度消失和梯度爆炸问题，我们需要详细推导反向传播的过程。首先，我们从输出层的参数w5开始，逐步推导每一层的梯...</div></div></div></a><a class="pagination-related" href="/2025/09/17/%E8%AE%BA%E6%96%87-Strong-Baseline-Multi-UAV-Tracking-via-YOLOv12-with-BoT-SORT-ReID/" title="论文-Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-17</div><div class="info-item-2">论文-Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID</div></div><div class="info-2"><div class="info-item-1">提出的方法以及数据集的缺陷 基于热红外视频的多无人机跟踪任务 该论文方法与 YOLOv5[18]和 DeepSORT[40]管道进行对比。使用了YOLOv12探测器和BoT-SORT-ReID pipline。此外还采用了一些方法提高了多无人机的跟踪性能。  图 (a)显示了来自 MOT 训练集的不同背景的热红外帧，而图 1 (b)突出了一些小缺陷，如注释错误、冗余、缺失标签和低质量帧，这些缺陷在数据集中占的比例可以忽略不计，在训练过程中可以安全地忽略。(反无人机比赛数据集) 普遍采用的提升方法 现有改进主要集中在 时间建模、实时优化、统一框架、检测后处理 上述四个方面，而作者的工作在此基础上结合最先进的检测器与跟踪器，在热红外多无人机跟踪上创造了新 benchmark，并指引后续研究。 详细方法 1、问题陈述 尽可能准确地跟踪无人机，评价指标： 提供初始位置的MOT任务。 2、数据 track3 YOLOv12 + BoT-SORT  YOLOv12  简单在线实时多目标跟踪技巧优化  BoT-SORT：卡尔曼滤波+运动相机补偿...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="https://cdn.jsdelivr.net/gh/Dekelkai/bucket@img/imgs/20250930204459119.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Dekel</div><div class="author-info-description">一个关于如何在快节奏的现代生活中，通过微小的改变和持续的自我探索，实现内心平静与个人成长的空间。</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">21</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">14</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Dekelkai"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/Dekelkai" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:tangxk666@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8%E5%B1%82encoder-layer"><span class="toc-number">1.</span> <span class="toc-text">编码器层（Encoder Layer）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8%E5%B1%82decoder-layer"><span class="toc-number">2.</span> <span class="toc-text">解码器层（Decoder Layer）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6attention"><span class="toc-number"></span> <span class="toc-text">注意力机制（Attention）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9Bself-attention"><span class="toc-number">1.</span> <span class="toc-text">自注意力（Self-Attention）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E8%AE%A1%E7%AE%97%E5%8D%95%E4%B8%AA"><span class="toc-number">2.</span> <span class="toc-text">自注意力的计算（单个）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E8%AE%A1%E7%AE%97%E7%9F%A9%E9%98%B5"><span class="toc-number">3.</span> <span class="toc-text">自注意力的计算（矩阵）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-number">4.</span> <span class="toc-text">多头注意力</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8encoder"><span class="toc-number"></span> <span class="toc-text">编码器（Encoder）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E8%A1%A8%E7%A4%BA%E5%BA%8F%E5%88%97%E7%9A%84%E4%BD%8D%E7%BD%AE"><span class="toc-number">1.</span> <span class="toc-text">使用位置编码表示序列的位置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%9A%84%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9Cfeed-forward-networks"><span class="toc-number">2.</span> <span class="toc-text">全连接的前馈网络（Feed-Forward
Networks）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AD%90%E5%B1%82%E4%B9%8B%E9%97%B4%E7%9A%84%E8%BF%9E%E6%8E%A5%E6%AE%8B%E5%B7%AE%E5%92%8C%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-number">3.</span> <span class="toc-text">子层之间的连接（残差和层归一化）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8E%9F%E5%A7%8B%E8%AE%BA%E6%96%87"><span class="toc-number">3.1.</span> <span class="toc-text">原始论文</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8decoder"><span class="toc-number"></span> <span class="toc-text">解码器（Decoder）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%90%8E%E7%9A%84%E7%BA%BF%E6%80%A7%E5%B1%82%E5%92%8Csoftmax%E5%B1%82"><span class="toc-number"></span> <span class="toc-text">最后的线性层和softmax层</span></a></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/09/30/%E5%89%8D%E7%AB%AF%E6%A1%86%E6%9E%B6%E7%9A%84%E8%B7%AF%E7%94%B1%E7%BB%84%E4%BB%B6%EF%BC%88router%EF%BC%89%E7%9A%84%E4%BD%9C%E7%94%A8/" title="前端框架的路由组件（router）的作用"><img src="https://txkimg.oss-cn-beijing.aliyuncs.com/blog/20250930175905113.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="前端框架的路由组件（router）的作用"/></a><div class="content"><a class="title" href="/2025/09/30/%E5%89%8D%E7%AB%AF%E6%A1%86%E6%9E%B6%E7%9A%84%E8%B7%AF%E7%94%B1%E7%BB%84%E4%BB%B6%EF%BC%88router%EF%BC%89%E7%9A%84%E4%BD%9C%E7%94%A8/" title="前端框架的路由组件（router）的作用">前端框架的路由组件（router）的作用</a><time datetime="2025-09-30T09:47:57.000Z" title="发表于 2025-09-30 17:47:57">2025-09-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/17/%E8%AE%BA%E6%96%87-Strong-Baseline-Multi-UAV-Tracking-via-YOLOv12-with-BoT-SORT-ReID/" title="论文-Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID">论文-Strong Baseline: Multi-UAV Tracking via YOLOv12 with BoT-SORT-ReID</a><time datetime="2025-09-17T08:51:13.000Z" title="发表于 2025-09-17 16:51:13">2025-09-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/10/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89-1%EF%BC%89%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%9C%80%E4%BC%98%E7%AD%96%E7%95%A5%E5%92%8C%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/" title="强化学习（三.1）贝尔曼最优策略和公式推导">强化学习（三.1）贝尔曼最优策略和公式推导</a><time datetime="2025-09-10T14:55:37.000Z" title="发表于 2025-09-10 22:55:37">2025-09-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%E7%8A%B6%E6%80%81%E5%80%BC%E4%B8%8E%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B/" title="强化学习（二）状态值与贝尔曼方程"><img src="https://txkimg.oss-cn-beijing.aliyuncs.com/blog/20250920085453909.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="强化学习（二）状态值与贝尔曼方程"/></a><div class="content"><a class="title" href="/2025/09/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89%E7%8A%B6%E6%80%81%E5%80%BC%E4%B8%8E%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B/" title="强化学习（二）状态值与贝尔曼方程">强化学习（二）状态值与贝尔曼方程</a><time datetime="2025-09-09T07:59:00.000Z" title="发表于 2025-09-09 15:59:00">2025-09-09</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/09/05/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89Basic%20Concepts/" title="强化学习（一）Basic Concepts">强化学习（一）Basic Concepts</a><time datetime="2025-09-04T16:07:41.000Z" title="发表于 2025-09-05 00:07:41">2025-09-05</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2023 - 2025 By Dekel</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.0</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>